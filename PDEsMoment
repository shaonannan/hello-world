import numpy as np
import scipy.linalg as spla
import scipy.stats as sps
import scipy.integrate as spi
import bisect
import itertools
import time



from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
import torch
from scipy.optimize import minimize
from scipy import special
import matplotlib.pyplot as plt


"""
02/02/2018 version
edited by SYX

initialization and configuration
vflag for dawson function 
fin for \rho_{Eq}
"""
####################################################################
V = np.linspace(-1.0,1.0,200)
dv = V[1]-V[0]
(vR,vmin) = (0.0,-2.0/3.0)
tmpvr = np.greater(V,vR)
indvr = np.squeeze(np.where(tmpvr))
tmpvm = np.greater(V,vmin)
indvm = np.squeeze(np.where(tmpvm))
vflag = np.ones((len(V),len(V)))
vflag[0:indvm[0],:]=0
vflag[indvm[0]:indvr[0],0:indvr[0]] = 0
for i in indvr:
    indexi = i
    vflag[indexi,0:indexi] = 0
vflag = np.transpose(vflag)
vflag = torch.Tensor(vflag)
fin     = np.ones((200,3))
for n in range(1,3):
    fin[:,n] = V*fin[:,n-1]
fin = torch.Tensor(fin)
"""
then we should release voltage, which may cause confused results for further 
analysis
"""
# V = np.reshape(V,[1,200])
# vedges = torch.Tensor(V)
vedges   = 0.0
#####################################################################

"""
03/27/2018 version
edited by SYX

utilities -- function
dawson function: 
    dawsncustom: exponential integration
    *scipy.special.dawsn
rho_Eq:
    rho_Eq --> torch.Tensor
    rho_EQRAW --> numpy
optfun:
    scipy.minimize
    gradient descent: torch.Tensor
get_v_edges:
    201 ---> 201 -1 = 200

"""
####################################################################
class ultilities():
    def dawsncustom(vvec,vflag,dv):
        texp2 = torch.Tensor.resize_(torch.exp(vvec**2),[1,200])
        return torch.squeeze(torch.mm(texp2,vflag)*dv)   
    def rho_Eq(vedg,Vs,D,vflag):
        sqrtD = torch.sqrt(D)
        dv = vedg[2]-vedg[1]
        Rv = torch.exp(-(vedg-Vs)**2/D)*ultilities.dawsncustom((vedg+dv/2.0-Vs)/sqrtD,vflag,dv/sqrtD)
        sum_c = dv*torch.sum(Rv)
        Rv = Rv/sum_c
        #print('sum_rhoEQ: ',sum_c)
        return torch.squeeze(Rv),Variable(torch.Tensor([sum_c]))
    def rho_EQRAW(Vs,D,V):
        Rv = np.copy(V)
        (vT,vR) = (1.0,0.0)
        tmpg = np.greater(V,vR)
        indp = (np.where(tmpg))
        sqrtD  = np.sqrt(D)

    
        intovT  = special.dawsn((vT-Vs)/sqrtD)*np.exp(np.square(vT-Vs)/D)
        intovSD = special.dawsn(-Vs/sqrtD)*np.exp(np.square(Vs)/D)

    
        Rv[indp] = -special.dawsn((V[indp]-Vs)/sqrtD)+np.exp(-np.square(V[indp]-Vs)/D)*intovT
        if(indp[0][0]>1):
            Rv[0:indp[0][0]] = np.exp(-np.square(V[0:indp[0][0]]-Vs)/D)*(-intovSD + intovT)
    
        tmpl = np.less(V,-2.0/3.0)
        indp = np.where(tmpl)
        Rv[indp] = 0.0
        sum_c = (V[2]-V[1])*np.sum(Rv)

        Rv = Rv/sum_c
        #print('RV_c: ',Rv)
        return torch.FloatTensor(np.float32(Rv)),1.0*sum_c
    def optfun(lambda_u,mu,x,Pq,fin,gamma):
        #lambda_u = lambda_u[:]
        fin = np.float64(fin)
        k  = np.size(mu)
        tt = np.zeros(k+1)
        tt[0] = 1
        tt[1:k+1]  = mu[:]
        
        dx = x[1]-x[0]
        # print 'dx: ',dx,'lambda: ',lambda_u
        # print 'lambda: ', lambda_u
        # lambda_u = lambda0[:]
        N  =np.size(lambda_u)
        # print N,np.shape(fin)
    
        p  = Pq*np.exp(np.dot(fin[:,0:N],lambda_u))
        #print('p: ',Pq)
        f  = dx*np.sum(p)-np.dot(np.reshape(tt,[1,k+1]),lambda_u)   
        #print('f: ',f)
        return f

    def optfuntorch(lambda_u,mu,x,Pq,fin,gamma):       
        k  = 2
        tt = Variable(torch.zeros(1,k+1))
        #print(mu)
        tt[0,0] = 1.0
        tt[0,1:] = mu[:]
        dx = x[1]-x[0]
        N  = 3
        #print(lambda_u)
        p  = Pq*torch.exp(torch.mm(torch.transpose(lambda_u,1,0),torch.transpose(fin,1,0)))
        f  = dx*torch.sum(p)-torch.mm(tt,lambda_u)   
        return f

    def get_v_edges(v_min,v_max,dv):
        edges = np.concatenate((np.arange(v_min,v_max,dv),[v_max]))
        edges[np.abs(edges) < np.finfo(np.float).eps] = 0
        Num = int(len(edges))
        return (Num),torch.Tensor(edges)
#############################################################################

"""
03/27/2018 version
edited by SYX

external population (sub-population)
represent mLGN inputs, real visual stimulus convolve spatiotemporal kernel, thus generating time trials
--> external population is equipped with ...
"""
#############################################################################
class ExternalPopulation(object):
    def __init__(self,firing_rate,dt,record=False,**kwargs):
        self.firing_rate_stream = Variable(torch.Tensor(firing_rate))
        self.firing_rate = Variable(torch.Tensor([0.0]))
        # may adding function to generate spike/firing rate trial use lambdify!!!
        self.type   = 'External'
        self.dt     = dt
        # additional data/parameters
        self.metadata = kwargs
        self.inmda  = Variable(torch.Tensor([0.0]))
        self.hnmda  = Variable(torch.Tensor([0.0]))
                              
        # for long-range connections
        self.tau_r  = 0.002*1e3
        self.tau_d  = 0.128*1e3
        self.curr_t = 0.0

        self.v1 = 0.0
        self.total_fp_sigv = Variable(torch.Tensor([0.0]))

        # initialize in simulation
        self.simulation = None
    def initialize(self):
        self.initialize_firing_rate()
    def update(self):
        self.update_firing_rate()
        self.curr_t += self.dt
                              
    def initialize_firing_rate(self):
        # current time --> in platform
        #self.curr_t = self.simulation.t      
        try:
            self.firing_rate = self.firing_rate_stream[np.int(self.curr_t/self.dt)]
        except:
            self.firing_rate = Variable(torch.Tensor([0.1]))
    def update_firing_rate(self):
        #self.curr_t = self.simulation.t       
        try:
            self.firing_rate = self.firing_rate_stream[np.int(self.curr_t/self.dt)]
        except:
            self.firing_rate = Variable(torch.Tensor([100.0]))

    # update own hNMDA and iNMDA, which only depends on curr_firing_rate 
    # in another words, a-subpopulation's hNMDA & iNMDA only depend on itself
    @property
    def curr_firing_rate(self):
        curr_firing_rate = self.firing_rate
        return curr_firing_rate
    @property
    def curr_Inmda(self):
        return self.inmda
#####################################################################################3

"""
02/02/2018 version
edited by SYX

internal population, neurons within primary visual cortex, which could receive recurrent synaptic inputs from 
other neurons/internal-populations in primary visual cortex.


"""   
    
class RecurrentPopulation(object):
    def __init__(self,tau_m = 20.0,dt = 0.1,v_min = -1.0,v_max = 1.0,dv = 1e-2,record = True,
                firing_rate = 0.0,fin = fin,**kwargs):
        # transmit parameters (variables!!! evolution / constant)
        
        # constant, wont change
        self.tau_m    = Variable(torch.Tensor([tau_m]))
        self.dt       = Variable(torch.Tensor([dt]))
        (self.v_min,self.v_max) = (v_min,v_max)
        self.dv       = dv
        self.fin      = Variable(torch.Tensor(fin))
        # additional parameters
        self.metadata = kwargs        
        # for long-range connections
        self.tau_r    = Variable(torch.Tensor([0.0020*1e3]))
        self.tau_d    = Variable(torch.Tensor([0.108*1e3]))
        self.type     = 'Recurrent'

        # variables
        # simulation in identical platform
        self.simulation = None
        self.edges = None
        self.tinput      = Variable(torch.Tensor([1.0]))
        self.Nedge       = Variable(torch.Tensor([0]))
        self.testw       = Variable(torch.Tensor([0.050]))
        self.firing_rate = Variable(torch.Tensor([0.0]))    
        # before real initialization, voltage-edge and voltage-distribution
        # are all None, these setting should be initialized later by specific command
        self.rhov        = Variable(torch.zeros(200))
        self.rhoEQ       = Variable(torch.zeros(200))
        self.sum_rhoEQ   = Variable(torch.zeros(1))      
        # if we use active release NMDA synaptic signal
        # once the sender(pre) population generated firing rate
        # it had ability to automatically release NMDA-type slow conductance
        # so it naturally has this property(without self.weights)

        # THIS IS FOR LR-CONN
        self.hnmda       = Variable(torch.zeros(1)) 
        self.inmda       = Variable(torch.zeros(1))  

        self.v1 = Variable(torch.zeros(1))
        self.v2 = Variable(torch.zeros(1))
        self.v3 = Variable(torch.zeros(1))
        self.v4 = Variable(torch.zeros(1))
        self.La0  = Variable(torch.zeros(4))#,requires_grad=True)
        self.total_fp_vslave = Variable(torch.zeros(1))
        self.total_fp_sigv   = Variable(torch.zeros(1))
       
        self.vpeak = Variable(torch.zeros(1))
    def initialize(self):
        """
        initialize some specific parameters and variables by hand
        with 
            1)voltage-edge/bin
            2)connection dictionary
            3)all about recorder
        """
        self.initialize_edges()
        self.initialize_prob()
        self.initialize_fpmusigv_dict()

        
    """
    Code below is designed for some basic matrix or elements which might be initialized
    at the beginning and maintained unchanged during the whole data analysis, but if U 
    need update some connections or strurtures, U could still start the 'Update' function 
    to regenerate a new structure(connections)
    """   
    def initialize_edges(self):
        # initialize discreted voltage bins
        self.Nedge,self.edges = ultilities.get_v_edges(self.v_min,self.v_max,self.dv)   # fixed

    def initialize_fpmusigv_dict(self):

        # dictionary for mu/sigma/Inmda

        self.total_fpmu_dict  = {}
        self.total_fpsig_dict = {}
        self.total_Inmda_dict = {}
        # identical for long-range connections
        for c in self.source_connection_list:
            if (c.conn_type =='ShortRange'):
                try:
                    curr_mu = self.total_fpmu_dict.setdefault(c.connection_distribution,0)
                    curr_sigv = self.total_fpsig_dict.setdefault(c.connection_distribution,0)
                except:
                    c.initialize_connection_distribution()
                    # then have name and signature
                    curr_mu = self.total_fpmu_dict.setdefault(c.connection_distribution,0)
                    curr_sigv = self.total_fpsig_dict.setdefault(c.connection_distribution,0)
                self.total_fpmu_dict[c.connection_distribution] =  curr_mu + c.curr_firing_rate * c.weights * c.nsyn# * (self.testw)
                self.total_fpsig_dict[c.connection_distribution] = curr_sigv + c.curr_firing_rate * (c.weights**2) * c.nsyn
            else:
                try:
                    curr_nmda_i = self.total_fpmu_dict.setdefault(c.connection_distribution,0)
                    #curr_nmda_i = self.total_fpsig_dict.setdefault(c.connection_distribution,0)
                except:
                    c.initialize_connection_distribution()
                    # then have name and signature
                    curr_nmda_i = self.total_fpmu_dict.setdefault(c.connection_distribution,0)
                    #curr_sigv = self.total_fpsig_dict.setdefault(c.connection_distribution,0)
                self.total_fpmu_dict[c.connection_distribution] =  curr_nmda_i + c.curr_Inmda * c.weights * c.nsyn# * (self.testw)
                self.total_Inmda_dict[c.connection_distribution] = curr_nmda_i + c.curr_Inmda * c.weights * c.nsyn# * (self.testw)

        
        # summation for only one value for mu/sigma/Inmda

        self.total_fp_vslave = Variable(torch.zeros(1))
        self.total_Inmda     = Variable(torch.zeros(1))
        for key,val in self.total_fpmu_dict.items():
            try:
                self.total_fp_vslave += val
            except:
                key.initialize()
                self.total_fp_vslave += val
        self.total_fp_vslave  = self.total_fp_vslave * self.tau_m

        # summation Inmda
        for key,val in self.total_Inmda_dict.items():  
            try:
                self.total_Inmda += val
            except:
                key.initialize()
                self.total_Inmda += val
        self.total_Inmda  = self.total_Inmda * self.tau_m
        

 
        # summation
        self.total_fp_sigv = Variable(torch.zeros(1))
        for key,val in self.total_fpsig_dict.items():
            try:
                self.total_fp_sigv += val
            except:
                key.initialize()
                self.total_fp_sigv += val
        self.total_fp_sigv  = self.total_fp_sigv * self.tau_m


    def initialize_prob(self):
        # initialize voltage-distribution
        self.rhov = torch.zeros(self.Nedge-1)
        # initiate moment
        vedges = self.edges
        h = vedges[2]-vedges[1]
        vedges = 0.5*(vedges[0:-1] + vedges[1:])
        
        var1   = torch.Tensor([(5/2/250.0)**2])
        source = torch.exp(-(vedges-0.0)**2/var1/2.0)/torch.sqrt(2.0*3.14*var1)
        source = source/(h*torch.sum(source))
        self.rhov = source
        rhovs = self.rhov
        

        v1 = h*torch.sum(vedges*rhovs)
        v2 = h*torch.sum(np.square(vedges)*rhovs)
        v3 = h*torch.sum(np.power(vedges,3.0)*rhovs)
        v4 = h*torch.sum(np.power(vedges,4.0)*rhovs)

        self.v1 = Variable(torch.Tensor([v1]))
        self.v2 = Variable(torch.Tensor([v2]))
        self.v3 = Variable(torch.Tensor([v3]))
        self.v4 = Variable(torch.Tensor([v4]))
        
        gamma   = torch.Tensor([1,v1,v2,v3,v4])
        fi      = Variable(torch.Tensor.resize_(gamma,[5,1]))
        F       = fi[1:3]
        La0     = fi[0:3]
        N       = 3  #3  !!!! number

        # saving self
        self.La0 = La0
        self.fin = fin
    # UPDATE LR NMDA SYNAPTIC INPUT
    def update_NMDA_midvar_syncurr(self):
        ownfr = self.curr_firing_rate
        # parameters
        deltat = self.dt
        trise  = self.tau_r
        tdamp  = self.tau_d

        tr   = deltat/trise
        etr  = torch.exp(-tr)
        td   = deltat/tdamp
        etd  = torch.exp(-td)
        cst  = 1.0/(tdamp - trise)*(etd - etr) # trise/(tdamp - trise)*(etd - etr)

        self.inmda = self.inmda * etd + self.hnmda * cst
        self.hnmda = self.hnmda * etr + ownfr * self.dt * 6.0 # * trise
        # print 'ownfr:  ',ownfr

    def update_total_fpmu_dict(self):

        # for curr_CD in self.source_connection_list:
        # have already exist
        for c in self.source_connection_list:
            self.total_fpmu_dict[c.connection_distribution] = 0.0

            if (c.conn_type=='LongRange'):
                self.total_Inmda_dict[c.connection_distribution]=0.0
                # also we should refresh Inmda_dict
                # and then real evolution of mu and sigma

        # have already clear up all the short range connections
        for c in self.source_connection_list:
            if(c.conn_type == 'ShortRange'):
                self.total_fpmu_dict[c.connection_distribution] += c.curr_firing_rate * c.weights * c.nsyn #1.0 * (self.testw)
            else:
                self.total_fpmu_dict[c.connection_distribution]  += c.curr_Inmda * c.weights * c.nsyn
                self.total_Inmda_dict[c.connection_distribution] += c.curr_Inmda * c.weights * c.nsyn
        
        # summation
        self.total_fp_vslave = 0.0
        for key,val in self.total_fpmu_dict.items():            
            try:
                self.total_fp_vslave += val
            except:
                key.initialize()
                self.total_fp_vslave += val
        self.total_fp_vslave = self.total_fp_vslave * self.tau_m

        # summation Inmda
        self.total_Inmda = 0.0
        for key,val in self.total_Inmda_dict.items():
            try:
                self.total_Inmda += val
            except:
                key.initialize()
                self.total_Inmda += val
        self.total_Inmda = self.total_Inmda * self.tau_m
        
        # print('total vslave: ',self.total_fp_vslave.data,' Inmda-vslave: ',self.total_Inmda.data)

    def update_input(self,tinputn):
        self.tinput = Variable(torch.Tensor([tinputn]))
        
    def update_total_fpsig_dict(self):

        for c in self.source_connection_list:
            self.total_fpsig_dict[c.connection_distribution] = 0.0
        for c in self.source_connection_list:
            if(c.conn_type == 'ShortRange'):
                self.total_fpsig_dict[c.connection_distribution] += c.curr_firing_rate * (c.weights**2) * c.nsyn #1.0 * (self.testw**2)

        # summation
        self.total_fp_sigv = 0.0
        for key,val in self.total_fpsig_dict.items():
            try:
                self.total_fp_sigv += val
            except:
                key.initialize()
                self.total_fp_sigv += val
        self.total_fp_sigv = self.total_fp_sigv * self.tau_m

    def update_fp_moment4(self):
        v1 = self.v1
        v2 = self.v2
        v3 = self.v3
        v4 = self.v4

        fr = self.firing_rate
        vs = self.total_fp_vslave
        ds = self.total_fp_sigv

        dtgL = self.dt / self.tau_m
        gL   = 1.0/self.tau_m
        
        self.v1 = v1 + dtgL*(-fr/gL - (v1-vs))
        self.v2 = v2 + dtgL*(-fr/gL - 2.0*(v2-vs*v1-0.5*ds))
        self.v3 = v3 + dtgL*(-fr/gL - 3.0*(v3-vs*v2-ds*v1))
        self.v4 = v4 + dtgL*(-fr/gL - 4.0*(v4-vs*v3-1.5*ds*v2))
        """
        matrix_iter = [[1.0,0.0,0.0,0.0,0.0],
                    [1.0*dtgL*vs,(1.0-1.0*dtgL),0.0,0.0,0.0],
                    [2.0*0.5*dtgL*ds,2.0*dtgL*vs,(1.0-2.0*dtgL),0.0,0.0],
                    [0.0,3.0*dtgL*ds,3.0*dtgL*vs,(1.0-3.0*dtgL),0.0],
                    [0.0,0.0,4.0*1.5*dtgL*ds,4.0*dtgL*vs,(1-4.0*dtgL)]
        dmaxtrix    = [[0.0,0.0,0.0,0.0,0.0],
                    [-self.dt*fr,0.0,0.0,0.0,0.0],
                    [-self.dt*fr,0.0,0.0,0.0,0.0],
                    [-self.dt*fr,0.0,0.0,0.0,0.0],
                    [-self.dt*fr,0.0,0.0,0.0,0.0]]
        tt_matrix_iter = matrix_iter + dmaxtrix
        v0 = torch.cat((torch.onese(1),v1,v2,v3,v4),0)
        v0 = torch.Tensor.resize_(v0,[5,1])
        vn = torch.squeeze(torch.mm(tt_matraix_iter,v0))
        self.v1 = vn[1]
        self.v2 = vn[2]
        self.v3 = vn[3]
        self.v4 = vn[4]
        """
    def update_rhoEQ(self):
        vs = self.total_fp_vslave
        ds = self.total_fp_sigv
        fin  = self.fin
        gL   = 1.0/self.tau_m

        h = self.edges[2]-self.edges[1]
        vedges = self.edges
        #print('edg: ',vedges)
        vedges = 0.5*(vedges[0:-1] + vedges[1:])
        #print('edg1: ',vedges)
        #rhoEQ,sum_rhoEQ = ultilities.rho_Eq(Variable(vedges),vs,ds,Variable(vflag))
        """
        if we use Variable(vedges),vs,ds,Variable(vflag) then, torch.Tensor.resize_ should use .data
        if we use vedges,vs.data,ds.data,vflag, we could directly use torch.Tensor.resize_ 
        """
        #rhoEQ,sum_rhoEQ = ultilities.rho_Eq((vedges),vs.data,ds.data,(vflag))
        rhoEQ,sum_rhoEQ = ultilities.rho_EQRAW(vs.data.numpy(),ds.data.numpy(),vedges.numpy())
        self.sum_rhoEQ  = sum_rhoEQ 
        self.rhoEQ      = rhoEQ
        
    def update(self):#_ME_moment4_early(self):
        self.update_NMDA_midvar_syncurr()
        self.update_total_fpsig_dict()
        self.update_total_fpmu_dict()
        self.update_fp_moment4()
        self.update_rhoEQ()
        # print 'Rec NMDA: ',self.curr_Inmda,self.inmda,' HNMDA: ',self.hnmda
        
        vs   = self.total_fp_vslave
        ds   = self.total_fp_sigv
        La0  = self.La0
        fin  = self.fin
        gL   = 1.0/self.tau_m

        self.La1  = La0
        h = self.edges[2]-self.edges[1]
        vedges = self.edges
        vedges = 0.5*(vedges[0:-1] + vedges[1:])
        rhoEQ  = self.rhoEQ 

        """
        THERE ARE TWO DIFFERENT WAYS TO FIND THE APPROPIRATE \RHO V,T
        1) MAXIMUM-ENTROPY WITH MOMENT CONSTRAINT
        2) GRADIENT DESCENT BY BACKPROPAGATION
        """
        
        gamma  = torch.cat((self.v1,self.v2,self.v3,self.v4),0)
        """
        this part could use two different approaches, 
        one
        """
        """
        fi     = np.transpose(gamma.data.numpy())
        F      = fi[0:2]
        (tmu,tx,tPEq,tfin,tgamma) = (F,vedges.numpy(),rhoEQ.numpy(),self.fin.numpy(),1.0)
        a0    = La0.data
        res   = minimize(ultilities.optfun,a0.numpy(),args=(tmu,tx,tPEq,tfin,tgamma))
        self.La0 = Variable(torch.Tensor(res.x))#indeed La0 is tensor or not make no difference
        La0 = self.La0
        """
        
        fi     = np.transpose(gamma.data.numpy())
        F      = fi[0:2]
        (tmu,tx,tPEq,tfin,tgamma) = (F,vedges.numpy(),rhoEQ.numpy(),self.fin.numpy(),1.0)
        a0    = La0.data
        res   = minimize(ultilities.optfun,a0.numpy(),args=(tmu,tx,tPEq,tfin,tgamma))
        self.La0 = Variable(torch.Tensor(res.x))#indeed La0 is tensor or not make no difference
        La0 = self.La0
        
        #later  
        rhov   = rhoEQ.view(200,1)*torch.exp(torch.matmul(torch.squeeze(self.fin),La0.data.view(3,1)))
        rhov   = (torch.squeeze(rhov))
        #print('sum_rhov: ',torch.sum(rhov))
        rhov   = rhov/(h*torch.sum(rhov))
        
        sum_rhoEQ   = self.sum_rhoEQ
        firing_rate = gL*torch.sqrt(ds)*torch.exp(torch.sum(torch.squeeze(self.La0)))/sum_rhoEQ/2.0
        self.rhov   = torch.squeeze(rhov)
        self.firing_rate = firing_rate
    
  
    @property
    def source_connection_list(self):
        return [c for c in self.simulation.connection_list if c.post_population == self]
        
    @property
    def curr_firing_rate(self):
        return self.firing_rate
    @property
    def curr_Inmda(self):
        return self.inmda


class Connection(object):
    def __init__(self,pre,post,nsyn,weights,probs,conn_type):
        self.pre_population  = pre
        self.post_population = post
        self.nsyn    = Variable(torch.Tensor([nsyn]))  # Number of Pre(sender) population
        self.weights = Variable(torch.Tensor([weights]))
        self.probs   = Variable(torch.Tensor([probs]))
        self.conn_type = conn_type
        # multiply probability of connection
        
        """
        1) connection_list should be classified into some unique population(cluster)
        which means, if 'weight''syn''prob' is identical,should be classified into identical 
        connection_distribution
        2) curr_firing_rate could be replace by ...
        3) simulation could be used to find original platform
        """
        # initialize None and Initialize when simulation
        self.firing_rate = Variable(torch.Tensor([0.0]))
        self.simulation  = None
        # long range
        self.inmda       = Variable(torch.Tensor([0.0]))
        """
        be remained!
        1) flux_matrix and threshold_flux_matrix,
        if connection has identical weight syn and prob, then the clux  matrix
        should be identical, this could be reuse --> connection_distribution
        """
    # initialize by hand! when start simulation
    def initialize(self):
        self.initialize_connection_distribution()
        self.initialize_firing_rate()
        self.initialize_I_nmda()
    
    def initialize_connection_distribution(self):
        CD = ConnectionDistribution(self.post_population.edges,self.weights,self.probs)
        CD.simulation = self.simulation
        self.simulation.connection_distribution_collection.add_unique_connection(CD)
        self.connection_distribution = self.simulation.connection_distribution_collection[CD.signature]
        
        
    def initialize_firing_rate(self):
        self.firing_rate = self.pre_population.curr_firing_rate
    # LONG RANGE 
    def initialize_I_nmda(self):
        self.inmda       = self.pre_population.curr_Inmda
        
    def update(self):
        self.firing_rate = self.pre_population.curr_firing_rate
        self.inmda       = self.pre_population.curr_Inmda
        # initialize_firing_rate
    def update_connection(self,npre,npost,nsyn,**nkwargs):
        self.pre_population = [],
        self.pre_population = npre,
        self.post_population = [],
        self.post_population = npost,
        self.syn_population = [],
        self.syn_population = nsyn
        
    @property
    def curr_firing_rate(self):
        return self.firing_rate
    @property
    def curr_Inmda(self):
        return self.inmda

class ConnectionDistribution(object):
    """
    Parameters:
    which could define unique connection,
    like weight, nsyn and prob
    may have synaptic delay
    
    Output pair
    """
    def __init__(self,edges,weights,probs,sparse = True):
        # all properties passed in are torch.Tensor/Variable
        self.edges   = edges   
        self.weights = weights
        self.probs   = probs

        # reversal potential could be used in conductance based model
        self.reversal_potential = None
        if self.reversal_potential != None:
            assert NotImplementedError 
    def initialize(self):
        self.t = 0.0

    @property    
    def signature(self):
        """
        unique signature
        """
        return (tuple(self.edges),tuple([self.weights]),tuple([self.probs]))           
            
class ConnectionDistributionCollection(dict):

    def add_unique_connection(self,cd):
        if not cd.signature in self.keys():
            self[cd.signature] = cd    
 
class Simulation(object):
    """
    Parameters:
    list :
        All sub-population (cluster)
        All connection (cluster)
        [type of both is 'List', which is changable variable, and could be changed]
        
    generate after initiate(by hand)
        connection_distribution
        connection_distribution_list
        [the differences between connection, connection_distribution and connection_distribution_list are
        connection ---> the component of 'connection_list', record all information and related information and object,like source and others
        connection_distribution --> this variable is a preparation variable for further processing, each 'connection' could generate a 
        class 'connecton_distribution' and then, using weight,syn,prob, could calculate flux_matrix and threshold
        each 'connection_distribution' item is defined by 'weight''syn ''prob', items with identical symbol will be classified to the same
        distribution
        connection_distribution_list --> this is a 'basket', store all unique connections(definition of unique: unique symbol
        'weight','syn','prob' no matter the target/source population)
    """
    def __init__(self,population_list,connection_list,verbose=True):
        
        self.verbose = verbose
        self.population_list = population_list
        self.connection_list = [c for c in connection_list if c.nsyn.data.numpy()!=0.0]
        self.m_record = np.zeros((40,10000))
    
    def initialize(self,t0=0.0):
        """
        initialize by hand, first put all sub-population and connection-pair
        !!! put them on the same platform!!! simulationBridge
        """
        
        # An connection_distribution_list (store unique connection(defined by weight,syn,prob))
        self.connection_distribution_collection = ConnectionDistributionCollection() # this is 
        self.t = t0
        
        # put all subpopulation and all connections into the same platform
        for subpop in self.population_list:
            subpop.simulation = self    # .simulation = self(self is what we called 'simulation')
        for connpair in self.connection_list:
            connpair.simulation = self
            
        # initialize population_list, calculate         
        for p in self.population_list:
            p.initialize()      # 2   
        
        for c in self.connection_list:
            #print 'initialize population'
            c.initialize()      # 1
            
    def update(self,t0 = 0.0,dt = 1e-3,tf = 1e-1):
        self.dt = dt#Variable(torch.Tensor([dt]))
        self.tf = tf#Variable(torch.Tensor([tf]))      
        # initialize:
        start_time = time.time()
        self.initialize(t0)
        self.initialize_time_period = time.time()-start_time
        
        # start_running
        start_time = time.time()
        counter = 0
        while self.t < self.tf:
            self.t+=self.dt
            ind_rec = 0
            #if self.verbose: print ('time: %s' % self.t)
            for p in self.population_list:
                p.update()
                ind_rec += 1
                if(ind_rec>24):
                    self.m_record[ind_rec-24,counter] = p.v1.data.numpy()
                if(np.mod(counter,100)==0):
                    if(ind_rec<=24):
                        print('External firing_rate: %.5f'%p.curr_firing_rate.data.numpy())  
                    else:
                        print('recurrent firing_rate: %.5f'%p.curr_firing_rate.data.numpy())  
            for c in self.connection_list:
                c.update()
            counter +=1
        return self.m_record


# intuitive network structure
Net_settings = {'hyp_num': 3,
                'xhyp':3,
                'yhyp':1,
                'xn':2,
                'yn':2}
# here we use orientation and phase
Fun_map_settings = {'ori_num':2,
                   'phase_num':10}
# cell numbers within identical orientation hypercolumn
Cell_type_num = {'e':50,
                'i':50}
print(Net_settings['hyp_num'])
ori_2d = np.zeros((Net_settings['hyp_num'] * Net_settings['xn'],Net_settings['yn']))
ori    = np.zeros(Net_settings['hyp_num'] * Net_settings['xn'] * Net_settings['yn'])
interval_ori = 180.0/Fun_map_settings['ori_num']
"""
this part is for generating orientation map (pin-wheel structure)
if np.mod(Net_settings['nx'],2) == 0,upper side cut off = np.floor(Net_settings['nx']/2), cen = cut-off - 0.5 
[0:cut-off] upper side; while [cut-off:] bottom side
if np.mod(Net_settings['nx'],2) == 1, upper side cut off = np.floor((Net_settings['nx']-1)/2), cen = cut-off
[0:cut-off] upper side; maintain unchanged [cut-off]; while bottom side [cut-off+1:]
"""
nbin = Net_settings['xn']
if np.mod(nbin,2) == 0:
    cut_off = np.floor(nbin/2.0)
    cen     = cut_off - 0.5
    (uc,up,bottom) = (int(cut_off),int(cut_off),int(cut_off))
if np.mod(nbin,2) == 1:
    cut_off = np.floor((nbin-1)/2.0)
    cen     = cut_off
    (uc,up,bottom) = (int(cut_off),int(cut_off),int(cut_off+1))
for i in np.arange(1):
    for p in np.arange(Net_settings['xn']):
        for q in np.arange(Net_settings['yn']):
            ttindex = q + p * Net_settings['yn'] + i * Net_settings['yn'] * Net_settings['xn']
            oriabs  = (np.arctan2(p-cen,q-cen)) * 180.0 / np.pi # ttindex
            if oriabs<0.0:
                oriabs = oriabs + 360.0
            oriabs = oriabs * 0.5
            oriabs = np.floor(oriabs/interval_ori)
            if oriabs == Fun_map_settings['ori_num']:
                oriabs = oriabs - 1
            ori[ttindex] = oriabs
            ori_2d[p,q]  = ori[ttindex]
"""
"""
# 2nd hypercolum!
# unchanged
ori_2d[Net_settings['xn']+uc,:] = ori_2d[uc,:]
start = 1*Net_settings['xn'] * Net_settings['yn']
ori_2d[Net_settings['xn']*1:Net_settings['xn']*1+up,:] = ori_2d[Net_settings['xn']-1:bottom-1:-1,:]
ori_2d[Net_settings['xn']*1+bottom:Net_settings['xn']*2,:] = ori_2d[up-1::-1,:]
ori[start:start+Net_settings['xn']*Net_settings['yn']] = np.squeeze(np.reshape(ori_2d[Net_settings['xn']*1:Net_settings['xn']*2,:],
                                                                   (1,Net_settings['xn']**2)))

# then here are two templates for 2*k / 2*k+1
# for 2*k
ori_template_2k  = np.squeeze(ori_2d[:Net_settings['xn'],:])
ori_template_2k1 = np.squeeze(ori_2d[Net_settings['xn']:2*Net_settings['xn'],:])

for ihyp in range(2,Net_settings['hyp_num']):
    start = ihyp*Net_settings['xn'] * Net_settings['yn']
    if np.mod(ihyp,2) == 0:
        ori_2d[Net_settings['xn']*ihyp:Net_settings['xn']*(ihyp+1),:] = ori_template_2k
    if np.mod(ihyp,2) == 1:
        ori_2d[Net_settings['xn']*ihyp:Net_settings['xn']*(ihyp+1),:] = ori_template_2k1
    ori[start:start+Net_settings['xn']*Net_settings['yn']] = np.squeeze(np.reshape(ori_2d[Net_settings['xn']*ihyp:Net_settings['xn']*(ihyp+1),:],
                                                                   (1,Net_settings['xn']**2)))
"""
"""
plt.figure()
plt.imshow(ori_2d)
plt.show()
def create_functional_columns(Structure_Net,Functional_Map,ori):
    nmax = Structure_Net['hyp_num'] * Structure_Net['xn'] * Structure_Net['yn']
    Structure_Net['nmax'] = nmax
    CGindex = np.arange(nmax)
    hypind  = np.arange(Structure_Net['hyp_num'])
    CGindx   = np.arange(Structure_Net['xn'])
    CGindy   = np.arange(Structure_Net['yn'])
    Orientation_map = {}
    Hypercol_map    = {}
    Hypercol        = np.zeros(nmax)
    Phase_map       = {}
    index_2_loc     = np.zeros((nmax,3))
    
    # orientation map
    for ihyp in hypind:
        for ix in CGindx:
            for iy in CGindy:
                ttindex = iy + ix * Structure_Net['yn'] + ihyp * Structure_Net['xn'] * Structure_Net['yn']
                Hypercol_map[(ttindex,'e')] = ihyp
                Hypercol_map[(ttindex,'i')] = ihyp
                Hypercol[ttindex]           = ihyp
                Phase_map[(ttindex,'e')] = np.random.randint(Fun_map_settings['phase_num'], size=1)
                Phase_map[(ttindex,'i')] = np.random.randint(Fun_map_settings['phase_num'], size=1)
                Orientation_map[(ttindex,'e')] = ori[ttindex]
                Orientation_map[(ttindex,'i')] = ori[ttindex]
                index_2_loc[ttindex,0] = ihyp
                index_2_loc[ttindex,1] = ihyp * Structure_Net['xn'] + ix
                index_2_loc[ttindex,2] = iy
    return (Structure_Net,Hypercol,Phase_map,Orientation_map,index_2_loc)

#******************** period distance **********************************************
def preprocess_for_distance_period(idex_2_loc,dxx,dyy,Net_settings,Fun_map_settings):
    nmax = Net_settings['nmax']
    # for X distance
    period_x = Net_settings['xn']
    global_x = np.reshape(np.squeeze(index_2_loc[:,1]),(nmax,1))
    global_xmatrix = np.repeat(global_x,nmax,axis=1)
    global_xdis    = np.abs(global_xmatrix - global_xmatrix.T)
    global_xdis    = np.minimum(global_xdis, period_x - global_xdis)**2*dxx
    # for Y distance
    period_y = Net_settings['yn']
    global_y = np.reshape(np.squeeze(index_2_loc[:,2]),(nmax,1))
    global_ymatrix = np.repeat(global_y,nmax,axis=1)
    global_ydis    = np.abs(global_ymatrix - global_ymatrix.T)
    global_ydis    = np.minimum(global_ydis, period_y - global_ydis)**2*dyy
    global_dist    = np.sqrt(global_xdis + global_ydis)
    
    return (global_xdis,global_ydis,global_dist)

# DISTANCE MATRIX ! BAC
#******************** cut-off distance **********************************************
def preprocess_for_distance(idex_2_loc,dxx,dyy,Net_settings,Fun_map_settings):
    nmax = Net_settings['nmax']
    # for X distance
    global_x = np.reshape(np.squeeze(index_2_loc[:,1]),(nmax,1))
    global_xmatrix = np.repeat(global_x,nmax,axis=1)
    global_xdis    = np.abs(global_xmatrix - global_xmatrix.T)**2*dxx
    # for Y distance
    global_y = np.reshape(np.squeeze(index_2_loc[:,2]),(nmax,1))
    global_ymatrix = np.repeat(global_y,nmax,axis=1)
    global_ydis    = np.abs(global_ymatrix - global_ymatrix.T)**2*dyy
    global_dist    = np.sqrt(global_xdis + global_ydis)
    
    return (global_xdis,global_ydis,global_dist)

def normal_function(x, mean=0, sigma=1.0):
    """
    Returns the value of probability density of normal distribution N(mean,sigma) at point `x`.
    """
    _normalization_factor = np.sqrt(2 * np.pi)

    #return np.exp(-np.power((x - mean)/sigma, 2)/2) / (sigma * _normalization_factor)
    return np.exp(-np.power((x - mean)/sigma, 2)/1.0) / (sigma * _normalization_factor) # do not divide 2.0

def circular_dist(a, b, period):
    """
    Returns the distance between a and b (scalars) in a domain with `period` period.
    """
    return np.minimum(np.abs(a - b), period - np.abs(a - b))
               
    

def create_network_population(Structure_Net,Functional_Map):
    #calculate total number of CG patches
    nmax    = Structure_Net['nmax']
    CGindex = np.arange(nmax)
    hypind  = np.arange(Structure_Net['hyp_num'])
    CGinx   = np.arange(Structure_Net['xn'])
    CGiny   = np.arange(Structure_Net['yn'])
    # Create populations:
    background_population_dict = {}
    internal_population_dict = {}
    for layer, celltype in itertools.product(CGindex, ['e', 'i']):    
        background_population_dict[layer, celltype] = 0
        internal_population_dict[layer, celltype] = 0

    return (Structure_Net,background_population_dict,internal_population_dict)

# MODIFYING 'nmax' in Net_settings, which is used in latter code
Net_settings,Hypm,Pham,Orim,index_2_loc = create_functional_columns(Net_settings,Fun_map_settings,ori)
"""
(dxx,dyy) = (800.0/Net_settings['xn'],800.0/Net_settings['yn'])
global_x,global_y,global_dist = preprocess_for_distance(index_2_loc,dxx,dyy,Net_settings,Fun_map_settings)

# only plot hypercolumn 0
start_hyp0 = Net_settings['xn'] * Net_settings['yn'] * 0
end_hyp0   = Net_settings['xn'] * Net_settings['yn'] * 1
local_x    = global_x[start_hyp0:end_hyp0,start_hyp0:end_hyp0]
local_y    = global_y[start_hyp0:end_hyp0,start_hyp0:end_hyp0]
"""
"""
using in real python code 2018/04/02 version 0
"""
# Simulation settings:
t0 = 0.0
dt = 1e-1
tf = 200.0
dv = 1e-2
verbose = True

update_method = 'approx'
approx_order = 1
tol  = 1e-14
# Create visual stimuli
def utilhvsvec(x):
    f = np.maximum(x,0)
    return f

def visual_temporal_kernel(t1,t2,dt,tfinal):
    tt = np.arange(0,tfinal,dt)
    f  = tt/(t1**2)*np.exp(-tt/t1) - tt/(t2**2)*np.exp(-tt/t2)
    return f

def input_convolution(t_pulse,dt,tfinal,nparam,tparam):
    ntt = int(tfinal/dt)
    npp = int(t_pulse/dt)
    square_pulse = np.ones(npp)
    t_resp       = np.zeros((nparam,ntt))
    # number of different temporal kernels
    for itk in range(nparam):
        t1 = tparam[itk,0]
        t2 = tparam[itk,1]
        tkernel = visual_temporal_kernel(t1,t2,dt,tfinal+5)
        t_k     = np.convolve(tkernel,square_pulse*dt,mode = 'full')
        t_resp[itk,:] = t_k[:ntt]
        # normalize
        t_resp[itk,:] = t_resp[itk,:] / np.max(np.squeeze(t_resp[itk,:]))
        t_resp[itk,:] = utilhvsvec(t_resp[itk,:])
        # only on pathway and off pathway, what we need is brightened and darken response(for stimuli's types), so we need 
        # brightened response equals to on-off pathway, while darken response equals to off-on pathway
        # slow on pathway, fast off pathway
        t_brightened = t_resp[0,:] - 0.6*t_resp[1,:]
        t_darken     = t_resp[1,:] - 0.6*t_resp[0,:]
        # heaviside function
        t_brightened = utilhvsvec(t_brightened)
        t_darken     = utilhvsvec(t_darken)

    return t_resp,t_brightened,t_darken


(dt, tfinal, t_pulse) = (dt, tf, 30)
nparam = 2
# (t1on,t2on,t1off,t2off) = (0.014/0.056*0.036,0.036,0.014/0.056*0.036,0.036)#(0.014,0.056,0.014/0.056*0.036,0.036)
tparam = np.zeros((nparam,2))
tparam[0,0] = 14.0
tparam[0,1] = 56.0
tparam[1,0] = 14.0/56.0*36.0
tparam[1,1] = 36.0

active_resp,active_brightened,active_darken = input_convolution(t_pulse,dt,tfinal,nparam,tparam)

# here, not only we use specific locations to receive visual stimuli, but also particular stimuli are chosen to 
# be assignedï¼ like assigning brightened and darken stimuli respectively.

stimuli_collection = {}
tt_delay = 20.0
tmp_brightened = np.zeros_like(active_brightened) 
tmp_darken     = np.zeros_like(active_darken) 
nnn = len(active_brightened)
nnt_delay = int(tt_delay/dt)

tmp_brightened[nnt_delay:] = active_brightened[:(nnn-nnt_delay)] 
active_brightened = tmp_brightened
tmp_darken[nnt_delay:] = active_darken[:(nnn-nnt_delay)] 
active_darken = tmp_darken

# artificial time delay
t_delay_brightened = 35.0
tmp_brightened = np.zeros_like(active_brightened) 
nnn = len(active_brightened)
nnt_delay = int(t_delay_brightened/dt)
tmp_brightened[nnt_delay:] = 0.88 *active_brightened[:(nnn-nnt_delay)] 
active_brightened = tmp_brightened

stimuli_collection['brightened'] = active_brightened
stimuli_collection['darken']     = active_darken


stimuli_type    = ['darken','brightened']
active_location = [-1,0,2]        # -1 --> represent index of hypercolumn   1 --> represent index of subpopulation
active_type     = ['b','b']       # 'e' represents excitatory only and 'i' represents inhibitory only
# while 'b' represents both excitatory and inhibitory populations could be stimulated

def input_signal_stream(active_location, active_type, active_resp,t_properties,Net_settings):
    (dt, tfinal) = t_properties

    ntt = int(tfinal/dt) # 20 to escape from error
    # Create base-visual stimuli
    External_stimuli_dict = np.ones(ntt)*0.150

    # Create populations:
    background_population_dict = {}
    internal_population_dict = {}
    CG_range = np.arange(Net_settings['nmax'])

    # base activities
    for layer, celltype in itertools.product(CG_range, ['e', 'i']):  
        background_population_dict[layer,celltype] = ExternalPopulation(External_stimuli_dict,dt, record=False)
        internal_population_dict[layer, celltype]  = RecurrentPopulation(dt = dt,v_min=-1.0, v_max=1.0, dv=dv, update_method=update_method, approx_order=approx_order, tol=tol)

    # active regions and corresponding activities
    Nactive = len(active_location) + 1
    if Nactive != len(active_type):
        print('! length of actived-location doesnt match with length of actived-type')
    # choose mode, hypercolumn or subpopulation
    mode = active_location[0]
    if(mode == -1): # hypercolumn
        for iactive in range(len(active_type)):
            # nomatter which particular location for target population,  the characteristics of 
            # particular stimuli should be determined already!
            # particular stimulus
            stm_type = stimuli_type[iactive]

            # particular target regions
            ihyper = active_location[iactive+1]
            ilocst = ihyper * Net_settings['xn'] * Net_settings['yn']
            iloced = (ihyper+1) * Net_settings['xn'] * Net_settings['yn']
            iloc_range = np.arange(ilocst,iloced)

            # cell types ? 'b' or 'e/i'
            celltype = active_type[iactive]
            if (celltype == 'b'):
                print('No.',iactive+1,' region \n','both excitatory and inhibitory subpopulations were stimulated! \n')
                particular_stimulus = stimuli_collection[stm_type]
                temp_inp = External_stimuli_dict[:ntt] + np.squeeze(particular_stimulus[:ntt])
                for ilayer, itype in itertools.product(iloc_range, ['e', 'i']):
                    # both excitatory and inhibitory
                    background_population_dict[ilayer,itype] = ExternalPopulation(temp_inp,dt,record=False)

            else:
                print('No.',iactive+1,' region \n','only ',celltype,' subpopulation was stimulated! \n')
                particular_stimulus = stimuli_collection[stm_type]
                temp_inp = External_stimuli_dict[:ntt] + np.squeeze(particular_stimulus[:ntt])
                for ilayer, itype in itertools.product(iloc_range, celltype):
                    # only subpopulation
                    background_population_dict[ilayer,itype] = ExternalPopulation(temp_inp,dt,record=False)



    """

    for iactive in range(len(active_type)):
        layer    = active_location[iactive]
        celltype = active_type[iactive]
        print(np.shape(External_stimuli_dict))
        print(np.shape(np.squeeze(active_resp[iactive,:ntt])))
        temp_inp = External_stimuli_dict[:ntt] + np.squeeze(active_resp[iactive,:ntt])
        background_population_dict[layer,celltype] = ExternalPopulation(temp_inp,dt,record=False)
    """
        
    population_list = list(background_population_dict.values()) + list(internal_population_dict.values())
    return (background_population_dict,internal_population_dict,population_list)

background_population_dict,internal_population_dict,population_list = input_signal_stream(active_location,active_type,active_resp,[dt,tfinal],Net_settings)
    
# PARAMETERS FOR CONNECTIVITY
(denexc,deninh,axnexc,axninh) = (50.0,50.0,200.0,100.0)
sr_exc2exc = np.sqrt(denexc * denexc + axnexc * axnexc)
sr_inh2exc = np.sqrt(denexc * denexc + axninh * axninh)
sr_exc2inh = np.sqrt(deninh * deninh + axnexc * axnexc)
sr_inh2inh = np.sqrt(deninh * deninh + axninh * axninh)

sr_sigma = {'short_range_exc_exc':sr_exc2exc,
           'short_range_exc_inh':sr_exc2inh,
           'short_range_inh_exc':sr_inh2exc,
           'short_range_inh_inh':sr_inh2inh} # source2target
lr_sigma = {'long_range_exc_exc': 1.00*1000,
            'long_range_exc_inh': 1.00*1000}

g_norm = {'short_range_exc_exc':0.24/50,
           'short_range_exc_inh':0.33/50,
           'short_range_inh_exc':-0.44/50,
           'short_range_inh_inh':-0.09/50,
           'long_range_exc_exc':0.236/50*1.0,
           'long_range_exc_inh':0.462/50*1.0} # source_target

# FUNCTIONS FOR CONNECTIVITY
def cortical_to_cortical_connection_normalization(global_dist,sr_sigma,lr_sigma,ori_map,hyp_map,Net_settings,Fun_map_settings):
    # for short-range connections normalization
    norm_cortical_connection = {}
    nmax    = Net_settings['nmax']
    # for long range connextions in different hypercolumns
    nori = Fun_map_settings['ori_num']
    # initialize
    norm_cortical_connection['lr','exc2exc'] = np.zeros((nmax,nmax))
    norm_cortical_connection['lr','exc2inh'] = np.zeros((nmax,nmax))
    """
    """
    for ihyp in range(Net_settings['hyp_num']):
        for iori in range(nori):
            index_hyper = np.where(hyp_map == ihyp)
            index_orien = np.where(ori_map == iori)
            index_target = np.intersect1d(index_hyper,index_orien)
            index_source = np.setdiff1d(index_orien,index_hyper)
            ee_curr = np.zeros((nmax,nmax))
            lentarget = len(index_target)
            lensource = len(index_source)
            # print(index_target,index_source)
            [x,y] = np.meshgrid(index_target,index_source)
            # print('hyp',ihyp,'ori',iori)
            A_temp  = (np.squeeze(global_dist[x,y])).T
            # plt.show()
            # print('dist',global_dist[12,:])
            # spimg = np.reshape(np.reshape(np.squeeze(global_dist[12,:]),(1,nmax)),(3*5,5))
            # plt.imshow(spimg)
            # plt.show()
            A_temp  = normal_function(A_temp,0,lr_sigma['long_range_exc_exc'])
            # print('Gaussian',A_temp)
            A_sum   = np.reshape(np.sum(A_temp,axis = 1),(lentarget,1))
            #print('sum',A_sum)
            A_sum   = np.ones_like(A_sum)/A_sum
            A_sum   = np.repeat(A_sum,lensource,axis = 1)
            ee_curr[x.T,y.T] = A_temp * A_sum 
            norm_cortical_connection['lr','exc2exc'] = norm_cortical_connection['lr','exc2exc'] + ee_curr
            
            ei_curr = np.zeros((nmax,nmax))
            lentarget = len(index_target)
            lensource = len(index_source)
            [x,y] = np.meshgrid(index_target,index_source)
            A_temp  = (np.squeeze(global_dist[x,y])).T
            A_temp  = normal_function(A_temp,0,lr_sigma['long_range_exc_inh'])
            A_sum   = np.reshape(np.sum(A_temp,axis = 1),(lentarget,1))
            A_sum   = 1.0/np.repeat(A_sum,lensource,axis = 1)
            ei_curr[x.T,y.T] = A_temp * A_sum 
            norm_cortical_connection['lr','exc2inh'] = norm_cortical_connection['lr','exc2inh'] + ei_curr
    """        
    """               
            
    # for short range connextions within identical hypercolumn
    nmax    = Net_settings['nmax']
    nmaxhyp = Net_settings['xn'] * Net_settings['yn']
    
    norm_cortical_connection['sr','exc2exc'] = np.zeros((nmax,nmax))
    norm_cortical_connection['sr','exc2inh'] = np.zeros((nmax,nmax))
    norm_cortical_connection['sr','inh2exc'] = np.zeros((nmax,nmax))
    norm_cortical_connection['sr','inh2inh'] = np.zeros((nmax,nmax))
    for ihyp in range(Net_settings['hyp_num']):
        index_start = nmaxhyp * ihyp 
        index_end   = nmaxhyp * (ihyp + 1)
        
        # exc to exc
        ee_curr = np.zeros((nmax,nmax))
        A_temp  = np.squeeze(global_dist[index_start:index_end,:])
        A_temp  = normal_function(A_temp,0,sr_sigma['short_range_exc_exc'])
        A_sum   = np.reshape(np.sum(A_temp,axis = 1),(nmaxhyp,1))
        A_sum   = 1.0/np.repeat(A_sum,nmax,axis = 1)
        A_norm  = (A_temp * A_sum) # x - target cell y source cell
        ee_curr[index_start:index_end,:] = A_norm
        norm_cortical_connection['sr','exc2exc'] = norm_cortical_connection['sr','exc2exc'] + ee_curr
        
        # exc to inh
        ei_curr = np.zeros((nmax,nmax))
        A_temp  = np.squeeze(global_dist[index_start:index_end,:])
        A_temp  = normal_function(A_temp,0,sr_sigma['short_range_exc_inh'])
        A_sum   = np.reshape(np.sum(A_temp,axis = 1),(nmaxhyp,1))
        A_sum   = 1.0/np.repeat(A_sum,nmax,axis = 1)
        A_norm  = (A_temp * A_sum) # x - target cell y source cell
        ei_curr[index_start:index_end,:] = A_norm
        norm_cortical_connection['sr','exc2inh'] = norm_cortical_connection['sr','exc2inh'] + ei_curr
        
        # inh to exc
        ie_curr = np.zeros((nmax,nmax))
        A_temp  = np.squeeze(global_dist[index_start:index_end,:])
        A_temp  = normal_function(A_temp,0,sr_sigma['short_range_inh_exc'])
        A_sum   = np.reshape(np.sum(A_temp,axis = 1),(nmaxhyp,1))
        A_sum   = 1.0/np.repeat(A_sum,nmax,axis = 1)
        A_norm  = (A_temp * A_sum) # x - target cell y source cell
        ie_curr[index_start:index_end,:] = A_norm
        norm_cortical_connection['sr','inh2exc'] = norm_cortical_connection['sr','inh2exc'] + ie_curr
        
        # inh to inh
        ii_curr = np.zeros((nmax,nmax))
        A_temp  = np.squeeze(global_dist[index_start:index_end,:])
        A_temp  = normal_function(A_temp,1,sr_sigma['short_range_inh_inh'])
        A_sum   = np.reshape(np.sum(A_temp,axis = 1),(nmaxhyp,1))
        A_sum   = 1.0/np.repeat(A_sum,nmax,axis = 1)
        A_norm  = (A_temp * A_sum) # x - target cell y source cell
        ii_curr[index_start:index_end,:] = A_norm
        norm_cortical_connection['sr','inh2inh'] = norm_cortical_connection['sr','inh2inh'] + ii_curr
        
    return norm_cortical_connection
    
def cortical_to_cortical_connection(background_population_dict, internal_population_dict, g_norm, delay, orientations,
                                    phases,cortical_norm):
    connection_list = []
    """
    """
    # feedforward connections
    for ihyp in range(Net_settings['hyp_num']):
        num_in_hyp = Net_settings['xn'] * Net_settings['yn']
        start_id = ihyp * num_in_hyp
        end_id   = (ihyp + 1) * num_in_hyp
        
        for ixs in range(Net_settings['xn']):
            for iys in range(Net_settings['yn']):
                sub_index_seed = iys + ixs * ( Net_settings['yn'])
                g_index_seed   = sub_index_seed + start_id
                
                # source_exc, target_exc
                source_population = background_population_dict[g_index_seed,'e']
                target_population = internal_population_dict[g_index_seed,'e']
                curr_connection = Connection(source_population,target_population,1.0, weights = 0.135,probs = 1.0,conn_type = 'ShortRange')
                connection_list.append(curr_connection)
                
                # source_inh, target_inh
                source_population = background_population_dict[g_index_seed,'i']
                target_population = internal_population_dict[g_index_seed,'i']
                curr_connection = Connection(source_population,target_population,1.0, weights = 0.136,probs = 1.0,conn_type = 'ShortRange')
                connection_list.append(curr_connection)
    """
    """
    # short-range connectivity nomalized 
    ge2e    = g_norm['short_range_exc_exc']
    ge2i    = g_norm['short_range_exc_inh']
    gi2e    = g_norm['short_range_inh_exc']
    gi2i    = g_norm['short_range_inh_inh']  
    gle2e   = g_norm['long_range_exc_exc']
    gle2i   = g_norm['long_range_exc_inh']              

    
    short_range_exc2exc = cortical_norm['sr','exc2exc']
    short_range_exc2inh = cortical_norm['sr','exc2inh']
    short_range_inh2exc = cortical_norm['sr','inh2exc']
    short_range_inh2inh = cortical_norm['sr','inh2inh']
    
    long_range_exc2exc  = cortical_norm['lr','exc2exc']
    long_range_exc2inh  = cortical_norm['lr','exc2inh']
    """
    """
    # self-connectivity (only when g_index_seed == target_index)
    for ihyp in range(Net_settings['hyp_num']):
        num_in_hyp = Net_settings['xn'] * Net_settings['yn']
        start_id = ihyp * num_in_hyp
        end_id   = (ihyp + 1) * num_in_hyp
        
        for ixs in range(Net_settings['xn']):
            for iys in range(Net_settings['yn']):
                sub_index_seed = iys + ixs * ( Net_settings['yn'])
                g_index_seed   = sub_index_seed + start_id
                i_up_triangle  = g_index_seed

                # source_exc, target_exc
                source_population = internal_population_dict[g_index_seed,'e']
                target_population = internal_population_dict[i_up_triangle,'e']
                curr_connection = Connection(source_population,target_population,Cell_type_num['e'], weights = ge2e * 
                                             short_range_exc2exc[i_up_triangle,g_index_seed], probs = 1.0,conn_type = 'ShortRange')
                connection_list.append(curr_connection)

                # source_exc, target_inh
                source_population = internal_population_dict[g_index_seed,'e']
                target_population = internal_population_dict[i_up_triangle,'i']
                curr_connection = Connection(source_population,target_population,Cell_type_num['e'], weights = ge2i * 
                                             short_range_exc2inh[i_up_triangle,g_index_seed], probs = 1.0,conn_type = 'ShortRange')
                connection_list.append(curr_connection)

                # source_inh, target_exc
                source_population = internal_population_dict[g_index_seed,'i']
                target_population = internal_population_dict[i_up_triangle,'e']
                curr_connection = Connection(source_population,target_population,Cell_type_num['i'], weights = gi2e *
                                             short_range_inh2exc[i_up_triangle,g_index_seed], probs = 1.0,conn_type = 'ShortRange')
                connection_list.append(curr_connection)

                # source_inh, target_inh
                source_population = internal_population_dict[g_index_seed,'i']
                target_population = internal_population_dict[i_up_triangle,'i']
                curr_connection = Connection(source_population,target_population,Cell_type_num['i'], weights = gi2i * 
                                             short_range_inh2inh[i_up_triangle,g_index_seed], probs = 1.0,conn_type = 'ShortRange')
                connection_list.append(curr_connection)
    """
    """
    # long-range connectivity 
    # choose excitatory as !!! source neuron !!!
    for ihyp in range(Net_settings['hyp_num']):
        num_in_hyp = Net_settings['xn'] * Net_settings['yn']
        start_id = ihyp * num_in_hyp
        end_id   = (ihyp + 1) * num_in_hyp
        
        for ixs in range(Net_settings['xn']):
            for iys in range(Net_settings['yn']):
                sub_index_seed = iys + ixs * ( Net_settings['yn'])
                g_index_seed   = sub_index_seed + start_id
                
                long_range_exc2exc  = cortical_norm['lr','exc2exc']
                long_range_exc2inh  = cortical_norm['lr','exc2inh']
                
                # long-range excitatory 2 excitatory
                # already know seed cell index
                # long-range connectivity [target,source]
                # choose excitatory subpopulation as source neuron
                total_exc2exc = np.squeeze(long_range_exc2exc[:,g_index_seed])
                total_exc2inh = np.squeeze(long_range_exc2inh[:,g_index_seed])
                # effective targetneuron
                effective_target_exc = np.where(total_exc2exc)
                effective_target_inh = np.where(total_exc2inh)
                
                for itarget in range(len(effective_target_exc[0])):
                    icell = effective_target_exc[0][itarget]
                    checkicell = effective_target_inh[0][itarget]
                    if(icell!=checkicell):
                        print('Cell-index for Long-range connectivity is wrong!\n')
                    source_population = internal_population_dict[g_index_seed,'e']
                    # excitatory target
                    target_population = internal_population_dict[icell,'e']
                    curr_connection = Connection(source_population,target_population,Cell_type_num['e'],
                                                 weights = gle2e * long_range_exc2exc[icell,g_index_seed], probs = 1.0, conn_type = 'LongRange' )
                    connection_list.append(curr_connection)
                    print('; No.',itarget,' target excitatory cell: ',icell)
                    # inhibitory target
                    target_population = internal_population_dict[checkicell,'i']
                    curr_connection = Connection(source_population,target_population,Cell_type_num['e'],
                                                 weights = gle2i * long_range_exc2inh[checkicell,g_index_seed], probs = 1.0, conn_type = 'LongRange')
                    connection_list.append(curr_connection)
                    print(' inhibitory cell: ',checkicell)
                print(' \n')
                    
                    
                """
                long_range_exc2exc  = cortical_norm['lr','exc2exc']
                long_range_exc2inh  = cortical_norm['lr','exc2inh']
                """

    """
    """
    nmax = Net_settings['hyp_num'] * Net_settings['xn'] * Net_settings['yn']
    # short-range connectivity, different patches, not 'delta' self-connectivity
    for ihyp in range(Net_settings['hyp_num']):
        num_in_hyp = Net_settings['xn'] * Net_settings['yn']
        start_id = ihyp * num_in_hyp
        end_id   = (ihyp + 1) * num_in_hyp
        
        for ixs in range(Net_settings['xn']):
            for iys in range(Net_settings['yn']):
                sub_index_seed = iys + ixs * ( Net_settings['yn'])
                g_index_seed   = sub_index_seed + start_id
                for i_up_triangle in range(g_index_seed+1,nmax):#end_id):

                    # source_exc, target_exc
                    source_population = internal_population_dict[g_index_seed,'e']
                    target_population = internal_population_dict[i_up_triangle,'e']
                    curr_connection = Connection(source_population,target_population,Cell_type_num['e'], weights = ge2e * 
                                                 short_range_exc2exc[i_up_triangle,g_index_seed],probs = 1.0,conn_type = 'ShortRange')
                    connection_list.append(curr_connection)
                    
                    # inverse
                    source_population = internal_population_dict[i_up_triangle,'e']
                    target_population = internal_population_dict[g_index_seed,'e']
                    curr_connection = Connection(source_population,target_population,Cell_type_num['e'], weights = ge2e * 
                                                 short_range_exc2exc[g_index_seed,i_up_triangle], probs = 1.0,conn_type = 'ShortRange')
                    connection_list.append(curr_connection)
                    
                    # source_exc, target_inh
                    source_population = internal_population_dict[g_index_seed,'e']
                    target_population = internal_population_dict[i_up_triangle,'i']
                    curr_connection = Connection(source_population,target_population,Cell_type_num['e'], weights = ge2i * 
                                                 short_range_exc2inh[i_up_triangle,g_index_seed], probs = 1.0,conn_type = 'ShortRange')
                    connection_list.append(curr_connection)
                    
                    source_population = internal_population_dict[i_up_triangle,'e']
                    target_population = internal_population_dict[g_index_seed,'i']
                    curr_connection = Connection(source_population,target_population,Cell_type_num['e'], weights = ge2i * 
                                                 short_range_exc2inh[g_index_seed,i_up_triangle], probs = 1.0,conn_type = 'ShortRange')
                    connection_list.append(curr_connection)
                    
                    # source_inh, target_exc
                    source_population = internal_population_dict[g_index_seed,'i']
                    target_population = internal_population_dict[i_up_triangle,'e']
                    curr_connection = Connection(source_population,target_population,Cell_type_num['i'], weights = gi2e * 
                                                 short_range_inh2exc[i_up_triangle,g_index_seed], probs = 1.0,conn_type = 'ShortRange')
                    connection_list.append(curr_connection)
                    
                    source_population = internal_population_dict[i_up_triangle,'i']
                    target_population = internal_population_dict[g_index_seed,'e']
                    curr_connection = Connection(source_population,target_population,Cell_type_num['i'], weights = gi2e * 
                                                 short_range_inh2exc[g_index_seed,i_up_triangle], probs = 1.0,conn_type = 'ShortRange')
                    connection_list.append(curr_connection)
                    
                    # source_inh, target_inh
                    source_population = internal_population_dict[g_index_seed,'i']
                    target_population = internal_population_dict[i_up_triangle,'i']
                    curr_connection = Connection(source_population,target_population,Cell_type_num['i'], weights = gi2i * 
                                                 short_range_inh2inh[i_up_triangle,g_index_seed], probs = 1.0,conn_type = 'ShortRange')
                    connection_list.append(curr_connection)
                    
                    source_population = internal_population_dict[i_up_triangle,'i']
                    target_population = internal_population_dict[g_index_seed,'i']
                    curr_connection = Connection(source_population,target_population,Cell_type_num['i'], weights = gi2i * 
                                                 short_range_inh2inh[g_index_seed,i_up_triangle], probs = 1.0,conn_type = 'ShortRange')
                    connection_list.append(curr_connection)
    """
    """        
    return connection_list


# Net_settings,Hypm,Pham,Orim,index_2_loc = create_functional_columns(Net_settings,Fun_map_settings,ori)
(dxx,dyy) = (1000.0/Net_settings['xn'],1000.0/Net_settings['yn'])
dxx = dxx**2
dyy = dyy**2
global_x,global_y,global_dist = preprocess_for_distance(index_2_loc,dxx,dyy,Net_settings,Fun_map_settings)
cortical_norm = cortical_to_cortical_connection_normalization(global_dist,sr_sigma,lr_sigma,ori,Hypm,Net_settings,Fun_map_settings)

long_range_exc2exc = cortical_norm['lr','exc2exc']
plt.figure()
plt.imshow(long_range_exc2exc)
plt.show()
connection_list = cortical_to_cortical_connection(background_population_dict,internal_population_dict,g_norm,0.0,Orim,Pham,
                                             cortical_norm)
import time
simulation = Simulation(population_list, connection_list, verbose=True)
m_record = simulation.update(dt=dt, tf=tf, t0=t0)


